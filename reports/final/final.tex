%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Template: Project Titlepage Modified (v 0.1) by rcx
%
% Original Source: http://www.howtotex.com
% Date: February 2014
%
% This is a title page template which be used for articles & reports.
%
% This is the modified version of the original Latex template from
% aforementioned website.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{report}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\graphicspath{ {img/} }
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{pgfgantt}
\usepackage{lscape}
\usepackage{url}
\usepackage{bm}
\setlength{\parindent}{0em}
\setlength{\parskip}{1.2em}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Similar nodes in large graphs}
\fancyhead[R]{Vlad Velici}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

%% USEFUL DEFINITIONS

\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{3}

\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}

\DeclareMathAlphabet{\mat}{OT1}{cmss}{bx}{12}

\begin{document}

\title{ \normalsize Electronics and Computer Science\\
Faculty of Physical Sciences and Engineering\\
University of Southampton
        \\ [2.0cm]
        Vlad Sebastian Velici \\
        \today
        \\ [1.0cm]
        {\LARGE \textbf{Similar nodes in large graphs}}
        \\ [1.0cm]
        Project Supervisor: Dr. Adam Pr√ºgel-Bennett \\
Second Examiner: Dr. Sasan Mahmoodi
	\\ [2.0cm]
	A project report submitted for the award of \\
MEng Computer Science with Artificial Intelligence
        \normalsize \vspace*{5\baselineskip}}

\date{}

\maketitle
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% ABSTRACT
%-------------------------------------------------------------------------------
\section*{Abstract}
%\addcontentsline{toc}{section}{Abstract}Given a large graph, in the range of millions or billions of nodes, it is difficult
to efficiently find similar nodes. Examples of applications of finding similar
nodes in data represented as graphs are all around the web nowadays:
\textit{people you may know} and \textit{who to follow} suggestions on social
networks, or topics that may interest you on different websites.


This project will explore ways to obtain meaningful information of this nature
from large datasets represented as graphs.
The goals of this project are developing efficient ways to find similar nodes in
large graphs for different use cases and implementing an open-source software
library to provide everyone with this functionality.
\newpage
%-------------------------------------------------------------------------------
% INTRODUCTION
%-------------------------------------------------------------------------------
\section{Introduction}
%\addcontentsline{toc}{section}{Introduction}
%
Plenty of datasets are or can be represented as graphs where vertices represent
entities and edges represent relationships between entities. A problem of interest
is to find entities that are similarly connected. Example instances of this
problem are finding \textit{people you may know} in a social network, people with
common interests from research publications repositories or identifying possible
duplicates in a dataset.

It is possible to calculate cloud vectors that represent the neighbourhoods of
vertices in a graph only by looking at the structure of the graph. Then if a cloud
vector is close to another it means the vertices they represent belong to a similar
neighbourhood. Using the dot product of cloud vectors we can compute different
similarity measures between vertices. Cosine similarity and Euclidean distance
are studied in this report.

It is computationally too expensive to compute all the cloud vectors and then
the dot products or the Euclidean distances. This project investigates methods
for approximating the dot products, different similarity measures based on the
dot products, and the evaluation of these measures on real world datasets.


%
%-------------------------------------------------------------------------------
% CLOUD VECTORS AND SIMILARITY MEASURES
%-------------------------------------------------------------------------------
%
\section{Cloud vectors and similarity measures}

The cloud vector of a vertex represents the way this vertex is connected to the
rest of the graph. In other words, it represents an extended neighbourhood of the
node. It is created by a process similar to diffusion, as shown in
\emph{Figure~\ref{fig:ci_diffusion}}.

Given a graph $\mathcal{G}(\mathcal{V}, \mathcal{E})$ with adjacency matrix
$\mat{M}$. The number of nodes is $N = |v|$ and the number of edges is
$|\epsilon|$.

The cloud vectors represent the neighbourhoods of vertices of the graph
$\mathcal{G}$, thus looking at the number of walks of different lengths from a
vertex $i$ to all the other vertices in the graph is a promising start.

The adjacency matrix has the property that if it is raised at the $k^{th}$ power,
the $(i, j)$ element of it, $\mat{M}^k_{ij}$, is the number of walks of length
$k$ from vertex $i$ to vertex $j$. Therefore the $i^{th}$ column of $\mat{M}^k$
is a vector that represents the way vertex $i$ is connected to the rest of the
graph by walks of lenght $k$.

The could vectors should include the walks of all lengths with a preference
towards the shorter walks over the longer walks. Therefore the adjacency matrix
should be normalised such that all rows sum up to one. Let
%
\begin{equation}
\label{eq:initial-d}
\mat{D}_{ij} = \frac{\mat{M}_{ij}}{\sum_k^N \mat{M}_{ik}}
\end{equation}
be the normalised adjacency matrix. A matrix that includes walks of all lengths
as described above can be written as the sum
%
\begin{equation}
\label{eq:mu^t_d^t}
\sum_t^\infty \mu^t \mat{D}^t,
\end{equation}
%
where $\mu \in [0,1]$ is a penalising factor. For each vertex of the graph, $i
\in \mathcal{V}$, the cloud vector is the $i^{th}$ column of the matrix in
\emph{Equation~\ref{eq:mu^t_d^t}}, or mathematically
%
\begin{equation}
\label{eq:brute-ci}
\bm{C}_i = \sum_{t=0}^\infty \mu^t D^t \bm{\delta}_i
\end{equation}
%
where $\bm{\delta}_i$ is the $i^{th}$ column of the $N \times N$ identity matrix.

\begin{figure}[tpb]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{frame0}
		\caption{$k=0$}
  \end{subfigure}%
  ~
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{frame1}
    \caption{$k=1$}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{frame2}
		\caption{$k=2$}
  \end{subfigure}%
  ~
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{frame3}
    \caption{$k=3$}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{frame4}
		\caption{$k=4$}
  \end{subfigure}%
  ~
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{frame5}
    \caption{$k=5$}
  \end{subfigure}


  \caption{ci sum calculation up to different k}
  \label{fig:ci_diffusion}
\end{figure}

%
%-------------------------------------------------------------------------------
% BRUTE FORCE
%-------------------------------------------------------------------------------
%
\section{Brute-force approach}
%\addcontentsline{toc}{chapter}{Brute-force approach}
%
The similarity algorithm is described regarding the undirected graph $G(v, \epsilon)$
with adjacency matrix $M$. The number of nodes is $N = |v|$ and the number of
edges is $|\epsilon|$.


In order to derive an algorithm to find similarities between vertices in a graph,
a measure of similarity between two vertices must be defined. $M$ is the adjacency
matrix of the graph and $M_{i j} = [(i,j) \in \epsilon]$, that is $M_{i j} = 1$
if there is an edge between vertices $i$ and $j$, otherwise $M_{i j} = 0$. Let
$D$ be a normalised adjacency matrix such that each row sums up to $1$:
%
\begin{equation}
D_{i j} = \frac{1}{\sum_{j=1}^N M_{ij}}
\end{equation}
%
Let $\mathbf{\delta_i}$ be the $i^{th}$ column of the $N \times N$ identity matrix,
and $\mathbf{C_i}$ a vector that represents how vertex $i$ is connected to the graph.
In a brute-force approach, $\mathbf{C_i}$ can be computed as:
%
\begin{equation}
\label{eq:brute-ci}
\mathbf{C_i} = \sum_{t=0}^\infty \mu^t D^t \mathbf{\delta_i}
\end{equation}
%
Where $\mu \in (0,1)$ is a penalising factor. The similarity between two vertices
$i$ and $j$ is the (squared) norm of the difference between $\mathbf{C_i}$ and
$\mathbf{C_j}$:
%
\begin{equation}
\|\mathbf{C_i} - \mathbf{C_j}\|^2
\end{equation}
%
The brute-force approach was run on a small graph designed by hand with 50 iterations
and it gave meaningful results (vertices connected in a similar way have a small
$\|\mathbf{C_i} - \mathbf{C_j}\|^2$) but it does not scale for large datasets. In
the next section an algorithm that scales up to large datasets is described.

%
%-------------------------------------------------------------------------------
% Approximative approach
%-------------------------------------------------------------------------------
%
\section{Approximative approach}
%\addcontentsline{toc}{section}{Approximative approach}
%
%% Geometry
%
Geometrically, if vectors $\mathbf{C_i}$ and $\mathbf{C_j}$ start from the origin,
then $\mathbf{C_i}$, $\mathbf{C_j}$ and $\mathbf{C_i}-\mathbf{C_j}$ describe a
triangle. Let $\theta$ be the angle between $\mathbf{C_i}$ and $\mathbf{C_j}$.
From the Law of Cosines:
%
\begin{equation}
\label{eq:cosinelaw}
\|\mathbf{C_i}-\mathbf{C_j}\|^2 = \|\mathbf{C_i}\|^2 + \|\mathbf{C_j}\|^2 - 2  \|\mathbf{C_i}\| \|\mathbf{C_j}\| \cos \ \theta
\end{equation}
%
Take the dot product $\mathbf{C_i^T}\mathbf{C_j} = \| \mathbf{C_i} \| \| \mathbf{C_j} \| \cos \theta$.
The angle between $\mathbf{C_i}$ and itself is $0$ and $\cos0=1$, thus
$\|\mathbf{C_i}\|^2 = \mathbf{C_i^T}\mathbf{C_i}$. Now \emph{Equation~\ref{eq:cosinelaw}} becomes:
%
\begin{equation}
\label{eq:norm-equation}
\|\mathbf{C_i}-\mathbf{C_j}\|^2 = \mathbf{C_i^T}\mathbf{C_i} + \mathbf{C_j^T}\mathbf{C_j} - 2 \mathbf{C_i^T}\mathbf{C_j}
\end{equation}
%
Therefore, it is sufficient to be able to compute all dot products $\mathbf{C_i^T}\mathbf{C_j}$
(for all $i, j \in v$) to compute the similarities between all vertices of the graph.
Computing and storing all $\mathbf{C_i^T}\mathbf{C_j}; i,j \in v$ takes too much time and memory
for a very large dataset, so we will make a trade-off between the accuracy of the result and
the computational complexity.


%
%% Approximative algorithm intro
%
An approximative algorithm to efficiently compute the dot products $\mathbf{C_i^T}\mathbf{C_j}$
will be described. Let $W$ be a diagonal matrix:
%
\begin{equation}
\label{eq:W}
W_{ij} = \frac{[i=j]}{\sum_{j=1}^N M_{ij}}
\end{equation}
%
Now \emph{Equation~\ref{eq:initial-d}} can be written as $D = WM$, thus
%
\begin{equation}
\label{eq:newdt}
D^t = (WM)^t
\end{equation}
%
Define a new matrix $A$:
%
\begin{equation}
\label{eq:a}
A = W^{\sfrac{1}{2}}MW^{\sfrac{1}{2}}
\end{equation}
%
%
From \emph{Equation~\ref{eq:newdt}} and \emph{Equation~\ref{eq:a}}:
%
\begin{equation}
\label{eq:used-dt}
D^t = W^{\sfrac{1}{2}} A^t W^{\sfrac{-1}{2}}
\end{equation}
%
%
The matrices $W^{\sfrac{1}{2}}$ and $W^{\sfrac{-1}{2}}$ are easy to compute because
$W$ is a diagonal matrix. Apply the operation only on the diagonal elements and obtain
$(W^{\sfrac{1}{2}})_{ii} = (W_{ii})^{\sfrac{1}{2}}$ and $(W^{\sfrac{-1}{2}})_{ii} = (W_{ii})^{\sfrac{-1}{2}}$.
%
%% Defining eigen*matrices.
%
Let $\lambda_i$ be the $i^{th}$ eigenvalue of $A$ and $\vee^{(i)}$ be the $i^{th}$
eigenvector of $A$ ($\forall i \in \{ 1, 2, ..., N \}$). $\vee$ is a matrix of all
$N$ eigenvectors (such that $\vee^{(i)}$ is the $i^{th}$ column of $\vee$), and
$\Lambda$ is a diagonal matrix of all $N$ eigenvalues:
%
\begin{align}
\vee &= \begin{bmatrix}
\vert      & \vert      &       & \vert \\
\vee^{(1)} & \vee^{(2)} & \dots & \vee^{(N)}\\
\vert      & \vert      &       & \vert \end{bmatrix} \\
\Lambda &= \begin{bmatrix}
\lambda_1 & 0 		      & \dots  & 0 \\
0 	 	    & \lambda_2   & \dots  & 0 \\
\vdots 	  & \vdots	    & \ddots &   \\
0	        & 0           &        & \lambda_N
\end{bmatrix}
\end{align}
%
%% Eigenvalue decomposition
%
The diagonalisation of the matrix A is $A = \vee \Lambda \vee^{-1}$. The graph $G$
is an undirected graph which makes $A$ a real symmetric matrix, thus $\vee^{-1} = \vee^{T}$
\cite{eigval_and_eigvec}.The diagonalisation of the matrix $A$ is therefore:
%
\begin{align}
\label{eq:a-at-t-sum}
A &= \vee \Lambda \vee^{T} \\
\Rightarrow A^t &= \big(\vee \Lambda \vee^{T}\big)^t = \vee \Lambda^t \vee^{T} \\
\Leftrightarrow A^t &= \sum_{a=1}^{N} \lambda_a^t \vee^{(a)} \vee^{(a)^T}
\end{align}
%
From \emph{Equation~\ref{eq:used-dt}} and \emph{Equation~\ref{eq:a-at-t-sum}}:
\begin{equation}
\label{eq:d-at-t-with-a}
D^t = W^{\sfrac{1}{2}} \sum_{a=1}^{N} \lambda_a^t \vee^{(a)} \vee^{(a)^T} W^{\sfrac{-1}{2}}
\end{equation}
%
%% Plugging into the Ci
%
Plug $D^t$ from \emph{Equation~\ref{eq:d-at-t-with-a}} into \emph{Equation~\ref{eq:brute-ci}}:
\begin{equation}
\mathbf{C_i} = \sum_{t=0}^\infty \mu^t W^{\sfrac{1}{2}} \sum_{a=1}^{N} \lambda_a^t \vee^{(a)} \vee^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i}
\end{equation}
Rearrange the equation and obtain:
\begin{equation}
\label{eq:ci-rearranged}
\mathbf{C_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{N} \sum_{t=0}^\infty \mu^t  \lambda_a^t \vee^{(a)} \vee^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i}
\end{equation}
%
%% Geometric series - to look into lambda mu < 1.
%
Note that $\sum_{t=0}^\infty \mu^t\lambda_a^t$ is a sum of a geometric series with
the first term $1$ and ratio $\mu \lambda_a$, therefore:
%
\begin{equation}
\label{eq:geometric-series}
\sum_{t=0}^\infty \mu^t\lambda_a^t = \frac{1}{1 - \mu \lambda_a}
\end{equation}
%
Observe $\vee^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i}$ is a scalar:
\begin{equation}
\label{eq:observe-scalar}
\vee^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i} = \vee^{(a)}_i W^{\sfrac{-1}{2}}_{ii}
\end{equation}
%
Substitute \emph{Equation~\ref{eq:observe-scalar}} and \emph{Equation~\ref{eq:geometric-series}}
back into \emph{Equation~\ref{eq:ci-rearranged}} and obtain:
%
\begin{equation}
\label{eq:ci-rearranged-geometric}
\mathbf{C_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{N} \frac{1}{1 - \mu \lambda_a} \vee^{(a)} \vee^{(a)}_i W^{\sfrac{-1}{2}}_{ii}
\end{equation}
%
%% Approximation explained.
%
An approximation of the vectors $\mathbf{C_i} (\forall i \in v)$ can be obtained
by only using the $m$ leading eigenvectors and eigenvalues of $A$ instead of all of
them. Compute the largest $m$ eigenvalues and eigenvectors of the matrix $A$ using
the Lanczos Method for real symmetric matrices\cite{calvetti1994implicitly}. For directed graphs the Arnoldi
Iteration \cite{lehoucq1996deflation} will be used instead. These algorithms perform well on large sparse
matrices. In real datasets it is very often the case that the number of edges is
orders of magnitude smaller than the number of vertices squared, resulting in sparse
adjacency matrices and a sparse $A$. Define $\mathbf{\hat{C}_i}$ to be an approximation of $\mathbf{C_i}$:
%
\begin{equation}
\label{eq:c-hat}
\mathbf{\hat{C}_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{m} \frac{1}{1 - \mu \lambda_a} \vee^{(a)} \vee^{(a)}_i W^{\sfrac{-1}{2}}_{ii}
\end{equation}
%
%% Define Z
%
To simplify the equation, let $Z$ be a $N \times m$ matrix such that:
\begin{equation}
\label{eq:z-ij}
Z_{ij} = \frac{\vee^{(j)}_i W^{\sfrac{-1}{2}}_{ii}}{1-\mu \lambda_j}
\end{equation}
%
Substitute in \emph{Equation~\ref{eq:c-hat}} and get:
%
\begin{equation}
\label{eq:ci-rearranged-z}
\mathbf{\hat{C}_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{N} Z_{ia} \vee^{(a)}
\end{equation}
%
The dot product between $\mathbf{\hat{C}_i}$ and $\mathbf{\hat{C}_j}$ becomes:
\begin{align}
%
\mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \Big(  W^{\sfrac{1}{2}} \sum_{a=1}^{m} Z_{ia} \vee^{(a)} \Big)^T \Big(  W^{\sfrac{1}{2}} \sum_{a'=1}^{m} Z_{ia'} \vee^{(a')} \Big)
\\
\Leftrightarrow \mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \sum_{a=1}^{m} Z_{ia}  \vee^{(a)^T} W \sum_{a'=1}^{m} Z_{ia'} \vee^{(a')}
\\
\Leftrightarrow \mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \sum_{a=1}^m \sum_{a'=1}^m Z_{ia} Z_{ja'} \vee^{(a)^T} W \vee^{(a')} \label{eq:dot-zvw}
\end{align}
%
%% Define Q
%
Define a new $m \times m$ matrix $Q$ such that:
%
\begin{equation}
\label{eq:q-ij}
Q_{ij} = \vee^{(i)^T} W \vee^{(j)}
\end{equation}
$\mathbf{Z_i}$ is the $i^{th}$ row of the matrix $Z$. Substitute $Q$ into \emph{Equation~\ref{eq:dot-zvw}} and obtain:
\begin{align}
\mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \sum_{a=1}^m \sum_{a'=1}^m Z_{ia} Z_{ja'} Q_{aa'} \label{eq:dot-zq-sum} \\
\Leftrightarrow \mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \mathbf{Z_i^T} Q \mathbf{Z_j} \label{eq:dot-zq-vectorised}
\end{align}
%
%% Storage and predictions
%
From \emph{Equation~\ref{eq:dot-zq-vectorised}} it is deduced that it is only
required to store the matrices $Q$ and $Z$, which only takes $O(m^2 + Nm)$ memory,
instead of $O(N^2)$ to store all $\mathbf{\hat{C}_i}$ vectors.


To compute the similarity between two vertices, $i$ and $j$, use the norm formula
from \emph{Equation~\ref{eq:dot-zq-vectorised}} and obtain:
%
\begin{equation}
\|\mathbf{\hat{C}_i^T} - \mathbf{\hat{C}_j}\|^2 = \mathbf{Z_i^T} Q \mathbf{Z_i} + \mathbf{Z_j^T} Q \mathbf{Z_j} - 2 \mathbf{Z_i^T} Q \mathbf{Z_j}
\end{equation}
%
After computing the matrices $Z$ and $Q$, the time complexity of computing the
similarity between two nodes is the same as multiplying three small matrices
(of sizes $1 \times m$, $m \times m$ and $m \times 1$).


%%%%%%%%%%%%%%%
%% Vectorisation
%%%%%%%%%%%%%%%
\subsection{Vectorisation}
%\addcontentsline{toc}{subsection}{Vectorisation}
It is usually more efficient to have vectorised implementations (to use matrix
operations in favour of iterative solutions) in programming languages like Matlab,
Octave or R. In this subsection is presented a way of computing the matrices $Z$
and $Q$ using matrix operations. For this purpose, in this subsection, $\vee$ only
contains the leading $m$ eigenvectors (is of size $N \times m$), and $\Lambda$ only
contains the leading $m$ eigenvalues (is of size $m \times m$).


%
%% Z
%
To vectorise the computation of $Z$ from \emph{Equation~\ref{eq:z-ij}}, define a
new diagonal matrix $R$:
%
\begin{equation}
R = \begin{bmatrix}
\frac{1}{1 - \mu \lambda_1} & 0  & \dots & 0 \\
0 & \frac{1}{1 - \mu \lambda_2}  & \dots & 0 \\
\vdots & \vdots & \ddots & \\
0 & 0 & \dots & \frac{1}{1 - \mu \lambda_m}
\end{bmatrix}
\end{equation}
And then define $Z$ as:
\begin{equation}
Z = W^{\sfrac{-1}{2}} \vee R
\end{equation}
%
%% Q
%
A vectorised version of \emph{Equation~\ref{eq:q-ij}} (to compute $Q$) is:
\begin{equation}
Q = \vee^T W \vee
\end{equation}


%
%-------------------------------------------------------------------------------
% IMPLEMENTATION
%-------------------------------------------------------------------------------
\section{Implementation}
%\addcontentsline{toc}{section}{Implementation}
%
A choice had to be made on what tools and languages to use for the implementation
of this algorithm. It involves linear algebra so programming languages that support
such operations or have good libraries for this purpose were candidates. Matlab
was chosen for this task because it has very strong linear algebra features built-in,
it is easy and fast to try different experiments (can also be used interactively)
and programs are usually smaller in size (lines of code) than their variants in C++
or other languages. At a later stage when the algorithm is closer to its final
version, porting it to a language like Go or C++ will be considered.


Matlab has the function \textit{eigs(A,m)} to compute the leading $m$ eigenvectors
and eigenvalues \cite{matlab:eigs}. Internally, it uses the software package ARPACK which implements
implicitly restarted Arnoldi methods. \cite{lehoucq1998arpack}


Sparse matrices were used for the implementation of both the approximative and
the brute-force approaches. A sparse matrix in Matlab only stores the non-zero
values of a matrix, therefore a sparse matrix only uses memory for the positions
where it holds values\cite{gilbert1992sparse}. For instance, to store the adjacency matrix of a very
large graph $G(v,\epsilon)$, using a dense matrix would take $O(|v|^2)$ memory
whereas using a sparse matrix only takes $O(|\epsilon|)$ memory. Sparse matrices
are faster at linear algebra operations.


The approximative approach was implemented in Matlab as a couple of Matlab functions.
The function signatures along with descriptions is presented in \emph{Table~\ref{tbl:interface}}.
Some helper functions are not included in the table.
%
\begin{table}[!ht]\footnotesize
   \centering
   \begin{tabular}{ll}
   similarity(adj, mu, m) & Computes and returns the matrices Q and Z. \\
   sim2(q,z,i,j) & Computes the similarity between vertices i and j. \\
   simlist(q,z,i) & Computes the similarity between vertex i and all other vertices. \\
   top(q,z,i) & Uses simlist(q,z,i) and sorts the results. \\
   randomGraph(nodes, vertices, directed) & Generates a random connected graph.
   \end{tabular}
   \caption{Implementation interface. Parameters: q represents the matrix $Q$, z represents the matrix $Z$, adj represents the adjacency matrix, and i and j represent vertex numbers.}
   \label{tbl:interface}
\end{table}


The brute-force algorithm was implemented as a separate set of Matlab functions
with a very similar interface.


%
%-------------------------------------------------------------------------------
% EVALUATION
%-------------------------------------------------------------------------------
%
\section{Evaluation}
%\addcontentsline{toc}{section}{Evaluation}
\subsection{Comparison of approximative and brute-force approaches}
%\addcontentsline{toc}{subsection}{Comparison of approximative and brute-force approaches}
%
A simple evaluation metric of how well the approximative approach retains the
most important information from the graph is to evaluate how different are the
top similar vertices for the brute-force approach and the approximative approach
with different number of eigenvalues and eigenvectors used.


For a dataset of $N$ vertices, let $B^m_{ij}$ be the position of vertex $j$ in
the top similar vertices of vertex $i$ for the brute-force approach with $m$
iterations. Similarly let $T^{m}_{ij}$ be the position of vertex $j$ in the top
similar vertices of vertex $i$ for the approximative approach with $m$ eigenvectors.
Define the difference between two such tops to be:
%
\begin{equation} d(B^m, T^{m'}) = \frac{1}{N^3} \sum_{i=1}^N \sum_{j=1}^N \operatorname{abs}(B^m_{ij} - T^{m'}_{ij}) \end{equation}
%
The equation is normalised by $N^3$ because the maximum value we can get lies in
the interval $(N^2, N^3)$.
%
\begin{figure}[tpb]
        \centering
        \begin{subfigure}[b]{0.6\textwidth}
	        \includegraphics[width=\textwidth]{no-of-eigenvectors}
        		\caption{A plot of the differences on a range of eigenvalues.}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.4\textwidth}
           \centering
           \def\arraystretch{1.5}
           \begin{tabular}{|c|c|}
	   \hline
	   Parameters & Value of d() \\
	   \hline
	   $d(B^{20}, T^{10})$ & 0.2501 \\
	   \hline
	   $d(B^{20}, T^{20})$ & 0.2257   \\
	   \hline
   	   $d(B^{20}, T^{30})$ & 0.2243   \\
	   \hline
	   $d(B^{20}, T^{40})$ & 0.2208   \\
	   \hline
	   $d(B^{20}, T^{50})$ & 0.2150 \\
	   \hline
	   $d(B^{20}, T^{100})$ & 0.2453\\
	   \hline
	    $d(B^{20}$, $T^{199})$ & 0.0325 \\
	    \hline
	   \end{tabular}
	   \caption{Table of some of the values.}
	   \label{tbl:comparisons}
        \end{subfigure}%
        \caption{Difference of results between brute-force and approximative approaches.}
	\label{fig:no-of-eigenvectors}
\end{figure}
%


The value of $d()$ was evaluated on a randomly generated connected graph with $200$
vertices and $350$ edges, between the brute-force approach with $m=20$ iterations
and the approximative approach with $m' = 10$ to $199$ eigenvectors. The choice
of the penalising factor $\mu=0.5$ was used for all the runs. The results can be
observed in \emph{Figure~\ref{fig:no-of-eigenvectors}}. We can observe that if
we use the majority of eigenvalues, the value of $d()$ is very small. It is
computationally infeasible to do that on a large dataset thus we will aim for a
number of eigenvectors $m'$ in the range of $20$ to $40$, where the difference
seems to be at a local minimum (for this graph). The local minimum differs from
dataset to dataset depending mainly on the size of the dataset. For completeness,
the experiment was run on a randomly generated connected graph with $500$ vertices
and $700$ edges. The first local minimum of $d()$ was at around 100 eigenvectors
with a value of approx. $0.2$. At $40$ and $20$ eigenvectors, the values of $d()$
obtained are $0.2297$ and $0.2557$, respectively.


%
% -----
% Evaluation of the algorithm itself
% -----
%
Although the evaluation metric described above might be useful to compare the two
approaches directly, it does not tell anything about how well the algorithm achieves
its goal of finding similar entities in a real dataset. We will now examine ways to
evaluate the performance of the algorithm on real problems.


%
\subsection{Visualisation}
%\addcontentsline{toc}{subsection}{Visualisation}
%
A subjective method of evaluation on real data is attempting to visualise the
algorithm output on real data and deduce whether it gives sensitive results.
This method has the disadvantage of being subjective and it cannot scale to very
large graphs, but it might give a broad idea of what the results are and help
fine-tune the algorithm on specific problems.


%
\subsection{Discussion of further evaluation}
%\addcontentsline{toc}{subsection}{Discussion of further evaluation}
%
None of the evaluation methods described above give an objective, efficient and
automatic evaluation metric. In this subsection we discuss a few possible evaluation
methods of the algorithm.


To objectively evaluate how well the outputs of the algorithm reflect the reality
of the dataset it is being used on, we need to carefully define the goals of the
algorithm in respect of the dataset. We want labelled datasets where the natural
\textit{similarities} between entities is known and depends exclusively on the
graph structure. We do not want to evaluate how well the weights of different
relationship types are defined, thus the evaluation dataset must have only one
relationship type.


Given some datasets that fit the above requirements, we can run our algorithm and
compare the results with the reference (natural) similarities. In reality, datasets
have many features and natural \textit{similarities} between entities depend on many
features, thus this type of evaluation is not practical.


For specific tasks we can evaluate the performance of our algorithm by adding or
removing some edges from the graph. For instance, if we want to find duplicates
we can create a dummy vertex by duplicating an existing one and randomly removing
some of its edges. We then evaluate whether the dummy vertex is very similar to
the original vertex.


Implementing the evaluation method described above is included in the future plans
of this project, along with investigating more methods of evaluation.


%
%-------------------------------------------------------------------------------
% LIMITATIONS
%-------------------------------------------------------------------------------
%
\section{Limitations of current implementation}
%\addcontentsline{toc}{section}{Limitations of the current implementation}
%
The current implementation of this algorithm has various limitations which are
discussed along with possible improvements.


Directed graphs are not currently supported. In practice, datasets have meaningful
unidirectional relationships (e.g. in a social network person A follows person B,
but B does not follow A), and often datasets are represented as directed graphs
rather than undirected graphs. The algorithm can be adapted to support both directed
and undirected graphs but it will have the disadvantage of requiring to compute
$V^{-1}$ (for undirected graphs, $V^{-1} = V^T$).


In the real world datasets might have different types of relationships between
entities. Some relationship types might be more relevant than others in finding
a specific result. For instance, in a social network two people being friends
might be more relevant for recommending new friends than two people following the
same topic. The current implementation of the algorithm can compute similarities
if it is given a weighted (symmetric) adjacency matrix but it does not have a
way to automatically obtain (learn) these weights from the dataset.


What if the dataset is too large to fit into main memory? The algorithm is
currently designed to run only on one machine, but finding ways to distribute it
over multiple machines will help run it on even larger datasets.

%
%-------------------------------------------------------------------------------
% FUTURE PLANS
%-------------------------------------------------------------------------------
%
\section{Future plans}
%\addcontentsline{toc}{section}{Future plans}
%
The highest priority improvements are: develop a good evaluation method, add support
for directed graphs, and use different edge weights for different types of relationships.
A detailed plan can be observed in the Gantt Chart in \emph{Figure~\ref{fig:gan}}.


An evaluation method is not trivial to design and build for this algorithm, but
it is crucial to the validation, further development and fine-tuning of the algorithm.
Researching an evaluation model has the highest priority in the list of tasks.


Plenty of datasets on which this algorithm can be applied describe directed graphs
with multiple types of relationships. The relevant tasks are the next tasks in
the Gantt Chart.


A demo on one or more datasets will be built. The first step into getting feedback
and evaluating how the algorithm performs on real data is to build an interface
where users can interact with the results.


A small research on how to make the algorithm run on multiple machines will be
done. Also, if the time allows, the code will be ported to a language like Go or
C++ and released as an open-source project.
\begin{landscape}
\begin{figure}[ft]
\begin{center}
\begin{ganttchart}[vgrid=true, hgrid=true, y unit chart=1.5em, y unit title=1.4em, title height=1]{1}{32}
\gantttitle{Semester 1}{17} \gantttitle{Semester 2}{15} \\
\gantttitlelist{1,...,32}{1} \\
\ganttbar{Agreed project \& brief}{1}{2} \\
\ganttbar{Implemented brute-force}{3}{3} \\
\ganttbar{Understanding math.}{4}{9} \\
\ganttbar{Implementing approx.}{8}{10} \\
\ganttbar{Evaluation ideas}{10}{11} \\
\ganttbar{Write progress report}{11}{11} \\
\ganttmilestone{Progress report}{11} \\
\ganttbar{Research evaluation}{12}{19} \\
\ganttbar{Directed graphs support}{18}{21} \\
\ganttbar{Learn weights algorithm}{21}{25} \\
\ganttbar{Real datasets demo}{26}{28} \\
\ganttbar{Write final report}{28}{30} \\
\ganttmilestone{Final report}{30} \\
\ganttbar{Prepare Viva}{31}{32} \\
\ganttmilestone{Viva}{32}
\end{ganttchart}
\end{center}
\caption{Gantt Chart of done and planned tasks over weeks.}
\label{fig:gan}
\end{figure}
\end{landscape}

\newpage
\addcontentsline{toc}{section}{Bibliography}
\bibliography{ref.bib}{}
\bibliographystyle{plain}
\end{document}
