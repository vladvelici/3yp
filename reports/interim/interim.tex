%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Template: Project Titlepage Modified (v 0.1) by rcx
%
% Original Source: http://www.howtotex.com
% Date: February 2014
% 
% This is a title page template which be used for articles & reports.
% 
% This is the modified version of the original Latex template from
% aforementioned website.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{report}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\graphicspath{ {img/} }
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{amsmath}
\usepackage{xfrac}
\setlength{\parindent}{0em}
\setlength{\parskip}{1.2em}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Similar nodes in large graphs}
\fancyhead[R]{Vlad Velici}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\begin{document}

\title{ \normalsize Electronics and Computer Science\\
Faculty of Physical Sciences and Engineering\\
University of Southampton 
        \\ [2.0cm]
        Vlad Sebastian Velici \\
        \today
        \\ [1.0cm]
        {\LARGE \textbf{Similar nodes in large graphs}}
        \\ [1.0cm]
        Project Supervisor: Dr. Adam Prügel-Bennett \\
Second Examiner: Dr. Sasan Mahmoodi
	\\ [2.0cm]
	A progress report submitted for the award of \\
MEng Computer Science with Artificial Intelligence
        \normalsize \vspace*{5\baselineskip}}

\date{}

\maketitle
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% Section title formatting
\sectionfont{\scshape}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% ABSTRACT
%-------------------------------------------------------------------------------
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}Given a large graph, in the range of millions or billions of nodes, it is difficult to efficiently find similar nodes. Examples of applications of finding similar nodes in data represented as graphs are all around the web nowadays: \textit{people you may know} and \textit{who to follow} suggestions on social networks, or topics that may interest you on different websites.
\par
This project will explore ways to obtain meaningful information of this nature from large datasets represented as graphs.
\parThe goals of this project are developing efficient ways to find similar nodes in large graphs for different use cases and implementing an open-source software library to provide everyone with this functionality.
\newpage
%-------------------------------------------------------------------------------
% INTRODUCTION
%-------------------------------------------------------------------------------
\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
%
Plenty of datasets are or can be represented as graphs where vertices represent entities and edges represent relationships between entities. A problem of interest is to find entities that are similarly connected. Example instances of this problem are finding \textit{people you may know} in a social network, people with common interests from research publications repositories or identifying possible duplicates in a dataset.
\par
It is not computationally feasible to calculate the exact similarities between vertices of very large graphs. In this project, we investigate a method to compute an approximation of similarities between vertices and attempt to evaluate its performance (accuracy) on different datasets.
\par

%
%-------------------------------------------------------------------------------
% BRUTE FORCE
%-------------------------------------------------------------------------------
%
\section*{Brute-Force Approach}
\addcontentsline{toc}{section}{Brute-Force Approach}
%
The similarity algorithm is described regarding the undirected graph $G(v, \epsilon)$ with adjacency matrix $M$. The number of nodes is $N = |v|$ and the number of edges is $|\epsilon|$.
%
In order to derive an algorithm to find similarities between vertices in a graph, a measure of similarity between two vertices must be defined. $M$ is the adjacency matrix of the graph and $M_{i j} = [(i,j) \in \epsilon]$, that is $M_{i j} = 1$ if there is an edge between vertices $i$ and $j$, otherwise $M_{i j} = 0$. Let $D$ be a normalised adjacency matrix such that each column sums up to $1$:
%
\begin{equation}
\label{eq:initial-d}
D_{i j} = \frac{1}{\sum_{j=1}^N M_{ij}}
\end{equation}
%
Let $\mathbf{\delta_i}$ be the $i^{th}$ column of the $N \times N$ identity matrix, and $\mathbf{C_i}$ a vector that represents how vertex $i$ is connected to the graph. In a brute-force approach, $\mathbf{C_i}$ can be computed as:
%
\begin{equation}
\label{eq:brute-ci}
\mathbf{C_i} = \sum_{t=0}^\infty \mu^t D^t \mathbf{\delta_i}
\end{equation}
%
Where $\mu \in (0,1)$ is a penalising factor. $\mu$ controls how much the far connections are considered into the similarities. Therefore, a large $\mu$ enforces consideration of very far connections whereas a small $\mu$ considers far connections less.
%
The similarity between two vertices $i$ and $j$ is the (squared) norm of the difference between $\mathbf{C_i}$ and $\mathbf{C_j}$:
%
\begin{equation}
\|\mathbf{C_i} - \mathbf{C_j}\|^2
\end{equation}
%
The brute-force approach was run on small randomly generated connected graphs (200 vertices) with 50 iterations and it gave meaningful results but it does not scale for large datasets. In the next section an algorithm that scales up to large datasets is described.
%
%-------------------------------------------------------------------------------
% Approximative approach
%-------------------------------------------------------------------------------
%
\section*{Approximative Approach}
\addcontentsline{toc}{section}{Approximative Approach}
%
%% Geometry 
%
Geometrically, if vectors $\mathbf{C_i}$ and $\mathbf{C_j}$ start from the origin, then $\mathbf{C_i}$, $\mathbf{C_j}$ and $\mathbf{C_i}-\mathbf{C_j}$ describe a triangle. Let $\theta$ be the angle between $\mathbf{C_i}$ and $\mathbf{C_j}$. From the Law of Cosines:
\begin{equation}
\label{eq:cosinelaw}
\|\mathbf{C_i}-\mathbf{C_j}\|^2 = \|\mathbf{C_i}\|^2 + \|\mathbf{C_j}\|^2 - 2  \|\mathbf{C_i}\| \|\mathbf{C_j}\| \cos \ \theta
\end{equation}
%
Take the dot product $\mathbf{C_i^T}\mathbf{C_j} = \| \mathbf{C_i} \| \| \mathbf{C_j} \| \cos \theta$. The angle between $\mathbf{C_i}$ and itself is $0$ and $\cos0=1$, thus $\|\mathbf{C_i}\|^2 = \mathbf{C_i^T}\mathbf{C_i}$. Now \emph{Equation~\ref{eq:cosinelaw}} becomes:
\begin{equation}
\label{eq:norm-equation}
\|\mathbf{C_i}-\mathbf{C_j}\|^2 = \mathbf{C_i^T}\mathbf{C_i} + \mathbf{C_j^T}\mathbf{C_j} - 2 \mathbf{C_i^T}\mathbf{C_j}
\end{equation}
%
Therefore, it is sufficient to be able to compute all dot products $\mathbf{C_i^T}\mathbf{C_j}$ (for all $i, j \in v$) to compute the similarities between all vertices of the graph. Computing and storing all $\mathbf{C_i^T}\mathbf{C_j}; i,j \in v$ takes too much time and memory for a very large dataset if we use the brute-force approach.
%
\par
%
%% Approximative algorithm intro
%
An approximative algorithm to efficiently compute the dot products $\mathbf{C_i^T}\mathbf{C_j}$ will be described. Let $W$ be a diagonal matrix:
%
\begin{equation}
\label{eq:W}
W_{ij} = \frac{[i=j]}{\sum_{j=1}^N M_{ij}}
\end{equation}
%
Now \emph{Equation~\ref{eq:initial-d}} can be written as $D = WM$, thus 
%
\begin{equation}
\label{eq:newdt}
D^t = (WM)^t
\end{equation}
%
Define a new matrix $A$:
%
\begin{equation}
\label{eq:a}
A = W^{\sfrac{1}{2}}MW^{\sfrac{1}{2}}
\end{equation}
%
%
From \emph{Equation~\ref{eq:newdt}} and \emph{Equation~\ref{eq:a}}:
%
\begin{equation}
\label{eq:used-dt}
D^t = W^{\sfrac{1}{2}} A^t W^{\sfrac{-1}{2}}
\end{equation}
%
%
The matrices $W^{\sfrac{1}{2}}$ and $W^{\sfrac{-1}{2}}$ are easy to compute because $W$ is a diagonal matrix. Apply the matrix operation only on the diagonal elements and obtain $(W^{\sfrac{1}{2}})_{ii} = (W_{ii})^{\sfrac{1}{2}}$ and $(W^{\sfrac{-1}{2}})_{ii} = (W_{ii})^{\sfrac{-1}{2}}$.
%
\par
%
%% Defining eigen*matrixes.
%
Let $\lambda_i$ be the $i^{th}$ eigenvalue and $\bigvee^{(i)}$ be the $i^{th}$ eigenvector ($\forall i \in \{ 1, 2, ..., N \}$). $\bigvee$ is a matrix of all computed eigenvectors (such that $\bigvee^{(i)}$ is the $i^{th}$ column of $\bigvee$), and $\bigwedge$ is a diagonal matrix of all computed eigenvalues:
\begin{align}
\bigvee      &=  \begin{bmatrix}
\vert & \vert &  & \vert \\
\bigvee^{(1)} & \bigvee^{(2)} & \dots & \bigvee^{(N)}\\
\vert & \vert &  & \vert \end{bmatrix} \\
\bigwedge &= \begin{bmatrix}
\lambda_1 & 0 		      & \dots  & 0\\
0 	 	 & \lambda_2  & \dots  &  0\\
\vdots 	 & \vdots	      & \ddots & \\
0	          & 0                  &            & \lambda_N
\end{bmatrix}
\end{align}
%
%% Eigenvalue decomposition
%
The eigendecomposition of the matrix A is $A = \bigvee \bigwedge \bigvee\nolimits^{-1}$. The graph $G$ is an undirected graph which makes $A$ a real symmetric matrix, thus $\bigvee\nolimits^{-1} = \bigvee\nolimits^{T}$. The eigendecomposition of the matrix $A$ is therefore:
\begin{align}
A &= \bigvee \bigwedge \bigvee\nolimits^{T} \\
\Rightarrow A^t &= \big(\bigvee \bigwedge \bigvee\nolimits^{T}\big)^t = \bigvee \bigwedge\nolimits^t \bigvee\nolimits^{T} \\
\Leftrightarrow A^t &= \sum_{a=1}^{N} \lambda_a^t \bigvee\nolimits^{(a)} \bigvee\nolimits^{(a)^T} \label{eq:a-at-t-sum} 
\end{align} 
%
From \emph{Equation~\ref{eq:used-dt}} and \emph{Equation~\ref{eq:a-at-t-sum}}:
\begin{equation}
\label{eq:d-at-t-with-a}
D^t = W^{\sfrac{1}{2}} \sum_{a=1}^{N} \lambda_a^t \bigvee\nolimits^{(a)} \bigvee\nolimits^{(a)^T} W^{\sfrac{-1}{2}}
\end{equation}
%
%% Plugging into the Ci
%
Plug $D^t$ from \emph{Equation~\ref{eq:d-at-t-with-a}} into \emph{Equation~\ref{eq:brute-ci}}:
\begin{equation}
\mathbf{C_i} = \sum_{t=0}^\infty \mu^t W^{\sfrac{1}{2}} \sum_{a=1}^{N} \lambda_a^t \bigvee\nolimits^{(a)} \bigvee\nolimits^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i}
\end{equation}
Rearrange the equation and obtain:
\begin{equation}
\label{eq:ci-rearranged}
\mathbf{C_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{N} \sum_{t=0}^\infty \mu^t  \lambda_a^t \bigvee\nolimits^{(a)} \bigvee\nolimits^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i}
\end{equation}
%
%% Geometric series - to look into lambda mu < 1.
%
Note that $\sum_{t=0}^\infty \mu^t\lambda_a^t$ is a sum of a geometric series with the first term $1$ and ratio $\mu \lambda_a$, therefore:
\begin{equation}
\label{eq:geometric-series}
\sum_{t=0}^\infty \mu^t\lambda_a^t = \frac{1}{1 - \mu \lambda_a}
\end{equation}
%
Observe $\bigvee\nolimits^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i}$ is a scalar:
\begin{equation}
\label{eq:observe-scalar}
\bigvee\nolimits^{(a)^T} W^{\sfrac{-1}{2}} \mathbf{\delta_i} = \bigvee\nolimits^{(a)}_i W^{\sfrac{-1}{2}}_{ii}
\end{equation}
%
Substitute \emph{Equation~\ref{eq:observe-scalar}} and \emph{Equation~\ref{eq:geometric-series}} back into \emph{Equation~\ref{eq:ci-rearranged}} and obtain:
\begin{equation}
\label{eq:ci-rearranged-geometric}
\mathbf{C_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{N} \frac{1}{1 - \mu \lambda_a} \bigvee\nolimits^{(a)} \bigvee\nolimits^{(a)}_i W^{\sfrac{-1}{2}}_{ii}
\end{equation}
%
%% Approximation explained.
%
An approximation of the vectors $\mathbf{C_i} (\forall i \in v)$ can be obtained by only using the $m$ leading eigenvectors and eigenvalues of $A$ instead of all of them. Compute the largest $m$ eigenvalues and eigenvectors of the matrix $A$ using the Lanczos Method for real symmetric matrices[1]. For directed graphs the Arnoldi Iteration[2] will be used instead. These algorithms perform well on large sparse matrices. In real datasets it is very often the case that the number of edges is orders of magnitude smaller than the number of vertices squared, resulting in sparse adjacency matrices and a sparse $A$. Define $\mathbf{\hat{C}_i}$ to be an approximation of $\mathbf{C_i}$:
\begin{equation}
\label{eq:c-hat}
\mathbf{\hat{C}_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{m} \frac{1}{1 - \mu \lambda_a} \bigvee\nolimits^{(a)} \bigvee\nolimits^{(a)}_i W^{\sfrac{-1}{2}}_{ii}
\end{equation}
%
%% Define Z
%
To simplify the equation, let $Z$ be a $N \times m$ matrix such that:
\begin{equation}
\label{eq:z-ij}
Z_{ij} = \frac{\bigvee\nolimits^{(j)}_i W^{\sfrac{-1}{2}}_{ii}}{1-\mu \lambda_j}
\end{equation}
%
Substitute in \emph{Equation~\ref{eq:c-hat}} and get:
%
\begin{equation}
\label{eq:ci-rearranged-z}
\mathbf{\hat{C}_i} = W^{\sfrac{1}{2}} \sum_{a=1}^{N} Z_{ia} \bigvee\nolimits^{(a)}
\end{equation}
%
The dot product between $\mathbf{\hat{C}_i}$ and $\mathbf{\hat{C}_j}$ becomes:
\begin{align}
%
\mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \Big(  W^{\sfrac{1}{2}} \sum_{a=1}^{m} Z_{ia} \bigvee\nolimits^{(a)} \Big)^T \Big(  W^{\sfrac{1}{2}} \sum_{a'=1}^{m} Z_{ia'} \bigvee\nolimits^{(a')} \Big)
\\
\Leftrightarrow \mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \sum_{a=1}^{m} Z_{ia}  \bigvee\nolimits^{(a)^T} W \sum_{a'=1}^{m} Z_{ia'} \bigvee\nolimits^{(a')}
\\
\Leftrightarrow \mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \sum_{a=1}^m \sum_{a'=1}^m Z_{ia} Z_{ja'} \bigvee\nolimits^{(a)^T} W \bigvee\nolimits^{(a')} \label{eq:dot-zvw}
\end{align}
%
%% Define Q
%
Define a new $m \times m$ matrix $Q$ such that:
\begin{equation}
\label{eq:q-ij}
Q_{ij} = \bigvee\nolimits^{(i)^T} W \bigvee\nolimits^{(j)}
\end{equation}
$\mathbf{Z_i}$ is the $i^{th}$ row of the matrix $Z$. Substitute $Q$ into \emph{Equation~\ref{eq:dot-zvw}} and obtain:
\begin{align}
\mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \sum_{a=1}^m \sum_{a'=1}^m Z_{ia} Z_{ja'} Q_{aa'} \label{eq:dot-zq-sum} \\
\Leftrightarrow \mathbf{\hat{C}_i^T} \mathbf{\hat{C}_j} &= \mathbf{Z_i^T} Q \mathbf{Z_j} \label{eq:dot-zq-vectorised}
\end{align}
%
%% Storage and predictions
%
From \emph{Equation~\ref{eq:dot-zq-vectorised}} it is deduced that it is only required to store the matrixes $Q$ and $Z$, which only takes $O(m^2 + Nm)$ memory, instead of $O(N^2)$ to store all $\mathbf{\hat{C}_i}$ vectors.
\par
To compute the similarity between two vertices, $i$ and $j$, use the norm formula from \emph{Equation~\ref{eq:dot-zq-vectorised}} and obtain:
\begin{equation}
\|\mathbf{\hat{C}_i^T} - \mathbf{\hat{C}_j}\|^2 = \mathbf{Z_i^T} Q \mathbf{Z_i} + \mathbf{Z_j^T} Q \mathbf{Z_j} - 2 \mathbf{Z_i^T} Q \mathbf{Z_j}
\end{equation}
After computing the matrices $Z$ and $Q$, the time complexity of computing the similarity between two nodes is the same as multiplying three small matrices (of sizes $1 \times m$, $m \times m$ and $m \times 1$).
%%%%%%%%%%%%%%%
%% Vectorisation
%%%%%%%%%%%%%%%
\subsection*{Vectorisation}
\addcontentsline{toc}{subsection}{Vectorisation}
It is usually more efficient to have vectorised implementations (to use matrix operations in favour of iterative solutions) in programming languages like Matlab, Octave or R. In this subsection is presented a way of computing the matrices $Z$ and $Q$ using matrix operations. For this purpose, in this subsection, $\bigvee$ only contains the computed eigenvectors (is of size $N \times m$), and $\bigwedge$ only contains the computed eigenvalues (is of size $m \times m$).
\par
%
%% Z
%
To vectorise the computation of $Z$ from \emph{Equation~\ref{eq:z-ij}}, define a new diagonal matrix $R$:
\begin{equation}
R = \begin{bmatrix}
\frac{1}{1 - \mu \lambda_1} & 0  & \dots & 0 \\
0 & \frac{1}{1 - \mu \lambda_2}  & \dots & 0 \\
\vdots & \vdots & \ddots & \\
0 & 0 & \dots & \frac{1}{1 - \mu \lambda_m}
\end{bmatrix}
\end{equation}
And then define $Z$ as:
\begin{equation}
Z = W^{\sfrac{-1}{2}} \bigvee R
\end{equation}
%
%% Q
%
A vectorised version of \emph{Equation~\ref{eq:q-ij}} (to compute $Q$):
\begin{equation}
Q = \bigvee\nolimits^T W \bigvee
\end{equation}
%
%-------------------------------------------------------------------------------
% IMPLEMENTATION
%-------------------------------------------------------------------------------
\section*{Implementation}
\addcontentsline{toc}{section}{Implementation}
%
A choice had to be made on what tools and languages to use for the implementation of this algorithm. It involves linear algebra so programming languages that support such operations or have good libraries for this purpose were candidates. Matlab was chosen for this task because it has very strong linear algebra features built-in, it is easy and fast to try different experiments (can also be used interactively) and programs are usually smaller in size (lines of code) than their variants in C++ or other languages. At a later stage when the algorithm is closer to its final version, porting it to a language like C++ will be considered.
\par
Matlab has the function \textit{eigs(A,m)} to compute the leading $m$ eigenvectors and eigenvalues. Internally, it uses the Lanczos Method for Hermitian matrices and the Arnoldi Iteration for non-Hermitian matrices.[3]
\par
For the implementation of both the approximative approach and the brute-force algorithm, sparse matrices were used. A sparse matrix in Matlab only stores the non-zero values of a matrix, therefore a sparse matrix only uses memory for the positions where it holds values.[4] For instance, to store the adjacency matrix of a very large graph $G(v,\epsilon)$, using a dense matrix would take $O(|v|^2)$ memory whereas using a sparse matrix only takes $O(|\epsilon|)$ memory.
\par
The approximative approach was implemented in Matlab as a couple of Matlab functions. The function signatures along with descriptions is presented in \emph{Table~\ref{tbl:interface}}.
\begin{table}[!ht]\footnotesize
   \centering
   \begin{tabular}{ll}
   similarity(adj, mu, m) & Computes and returns the matrices Q and Z. \\
   sim2(q,z,i,j) & Computes the similarity between vertices i and j. \\
   simlist(q,z,i) & Computes the similarity between vertex i and all other vertices. \\
   top(q,z,i) & Uses simlist(q,z,i) and sorts the results. \\
   randomGraph(nodes, vertices, directed) & Generates a random connected graph. \\
   readEprints() & Reads ePrints data from a pre-processed file.
   \end{tabular}
   \caption{Implementation interface. Parameters: q represents the matrix $Q$, z represents the matrix $Z$, adj represents the adjacency matrix, and i and j represent vertex numbers.}
   \label{tbl:interface}
\end{table}
\par
The brute-force algorithm was implemented as a separate set of Matlab functions with a very similar interface.
%
%-------------------------------------------------------------------------------
% EVALUATION
%-------------------------------------------------------------------------------
\section*{Evaluation}
\addcontentsline{toc}{section}{Evaluation}
\subsection*{Comparison of approximative and brute-force approaches}
\addcontentsline{toc}{subsection}{Comparison of approximative and brute-force approaches}
A simple evaluation metric of how well the approximative approach retains the most important information from the graph is to evaluate how different are the top similar vertices for the brute-force approach and the approximative approach with different number of eigenvalues and eigenvectors used.
\par
For a dataset of $N$ vertices, let $B^m_{ij}$ be the position of vertex $j$ in the top similar vertices of vertex $i$ for the brute-force approach with $m$ iterations. Similarly let $T^{m}_{ij}$ be the position of vertex $j$ in the top similar vertices of vertex $i$ for the approximative approach with $m$ eigenvectors. Define the difference between two such tops to be: 
\begin{equation} d(B^m, T^{m'}) = \frac{1}{N^3} \sum_{i=1}^N \sum_{j=1}^N \operatorname{abs}(B^m_{ij} - T^{m'}_{ij}) \end{equation}
%
The equation is normalised by $N^3$ because the maximum value we can get lies in the interval $(N^2, N^3)$.
%
\begin{figure}[tpb]
        \centering
        \begin{subfigure}[b]{0.6\textwidth}
	        \includegraphics[width=\textwidth]{no-of-eigenvectors}
        		\caption{A plot of the differences on a range of eigenvalues.}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.4\textwidth}
           \centering
           \def\arraystretch{1.5}
           \begin{tabular}{|c|c|}
	   \hline
	   Parameters & Value of d() \\
	   \hline
	   $d(B^{20}, T^{10})$ & 0.2501 \\
	   \hline
	   $d(B^{20}, T^{20})$ & 0.2257   \\ 
	   \hline
   	   $d(B^{20}, T^{30})$ & 0.2243   \\ 
	   \hline
	   $d(B^{20}, T^{40})$ & 0.2208   \\ 
	   \hline
	   $d(B^{20}, T^{50})$ & 0.2150 \\
	   \hline
	   $d(B^{20}, T^{100})$ & 0.2453\\
	   \hline
	    $d(B^{20}$, $T^{199})$ & 0.0325 \\
	    \hline
	   \end{tabular}
	   \caption{Table of some of the values.}
	   \label{tbl:comparisons}
        \end{subfigure}%
        \caption{Difference of results between brute-force and approximative approaches.}
	\label{fig:no-of-eigenvectors}
\end{figure}
%
The results can be observed in \emph{Figure~\ref{fig:no-of-eigenvectors}}. The choice of the penalising factor $\mu=0.5$ was used for all the runs. The difference d was evaluated between the brute-force approach with $m=20$ iterations and the approximative approach with $m' = 5 to 195$ eigenvectors. We can observe that if we use the majority of eigenvalues, the value of $d()$ is very small. It is computationally infeasible to do that on a large dataset thus we will aim for a number of eigenvectors $m'$ in the range of $20$ to $40$, where the difference seems to be at a local minimum (on this small dataset). The local minimum might differ from dataset to dataset depending mainly on the size of the dataset. For completeness, the experiment was run on a randomly generated connected graph with $500$ vertices and $700$ edges. The first local minimum of $d()$ was at around 100 eigenvectors with a value of approx. $0.2$. At $40$ and $20$ eigenvectors, the values of $d()$ obtained are $0.2297$ and $0.2557$, respectively.
%
%
% -----
% Evaluation of the algorithm itself
% -----
%
\par
Although the evaluation metric described above might be useful to compare the two approaches directly, it does not tell anything about how well the algorithm achieves its goal of finding similar entities in a real dataset. Even if the computation of the brute-force approach would be feasible on large datasets for comparison, it would still not verify the underlying intuition of this algorithm.
%
\subsection*{Visualisation}
\addcontentsline{toc}{subsection}{Visualisation}
%
A subjective method of evaluation on real data is attempting to visualise the algorithm output on real data and deduce whether it gives sensitive results. This method has the disadvantage of being subjective and it cannot scale to very large graphs.
%
\subsection*{Discussion of further evaluation}
\addcontentsline{toc}{subsection}{Discussion of further evaluation}
%
None of the evaluation methods described above give an objective, efficient and automatic evaluation metric. In this subsection we discuss a few possible evaluation methods of the algorithm.
\par
To objectively evaluate how well the outputs of the algorithm reflect the reality of the dataset it is being used on, we need to carefully define the goals of the algorithm in respect of the dataset. We want labelled datasets where the natural \textit{similarities} between entities is known and depends exclusively on the graph structure. We do not want to evaluate how well the weights of different relationship types are defined, thus the evaluation dataset must have only one relationship type.
\par
Given some datasets that fit the requirements from above, we can run our algorithm and compare the results with the reference similarities. In reality datasets have many features and natural \textit{similarities} between entities depend on many features, thus this type of evaluation is not practical.
\par
For specific tasks we can evaluate the performance of our algorithm by adding or removing some edges from the graph. For instance, if we want to find duplicates we can create a dummy vertex by duplicating an existing one and randomly removing some of its edges. We then evaluate whether the dummy vertex is very similar to the original vertex.
\par
Implementing the evaluation method described above is included in the future plans of this project, along with investigating more methods of evaluation.
%
%-------------------------------------------------------------------------------
% LIMITATIONS
%-------------------------------------------------------------------------------
%
\section*{Limitations of Current Implementation}
\addcontentsline{toc}{section}{Limitations of the Current Implementation}
The current implementation of this algorithm has various limitations which are discussed along with possible improvements.
%
\paragraph*{Only undirected graphs} Directed graphs are not currently supported. In practice, datasets have meaningful unidirectional relationships (e.g. in a social network person A follows person B, but B does not follow A), and often datasets are represented as directed graphs rather than undirected graphs. The algorithm can be adapted to support both directed and undirected graphs but it will have the disadvantage of requiring to compute $V^{-1}$ (for undirected graphs, $V^{-1} = V^T$).
%
\paragraph*{Ignored relationship types} In the real world datasets might have different types of relationships between entities. Some relationship types might be more relevant than others in finding a specific result. For instance, in a social network two people being friends might be more relevant for recommending new friends than two people following the same topic. The current implementation of the algorithm can compute similarities if it is given a weighted (symmetric) adjacency matrix but it does not have a way to automatically obtain these weights from the dataset.
%
\paragraph*{Not distributed} What if the dataset is too large to fit into main memory? The algorithm is currently only designed to run on one machine. Methods of distributing the algorithm on more than one machine over a network will be investigated in the future.
%
\paragraph*{Weights of edges} To compute similarities between nodes in real datasets we often need to consider more features. Each feature can be described as a type of relationship that leads to edges of different weights in the graph. Nothing is currently implemented to help learn the optimal weights from data to maximise the performance of the algorithm.

%-------------------------------------------------------------------------------
% REFERENCES
%-------------------------------------------------------------------------------
\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

[1]: Calvetti, Daniela, L. Reichel, and Danny Chris Sorensen. "An implicitly restarted Lanczos method for large symmetric eigenvalue problems." Electronic Transactions on Numerical Analysis 2.1 (1994): 21.

[2]: Lehoucq, Richard B., and Danny C. Sorensen. "Deflation techniques for an implicitly restarted Arnoldi iteration." SIAM Journal on Matrix Analysis and Applications 17.4 (1996): 789-821.

[3]: "Matlab 2014b Documentation." Largest Eigenvalues and Eigenvectors of Matrix. The MathWorks, Inc., 3 Oct. 2014. Web. Accessed 07 Dec. 2014.\newline <http://uk.mathworks.com/help/matlab/ref/eigs.html>.

[4]: Gilbert, John R., Cleve Moler, and Robert Schreiber. "Sparse matrices in MATLAB: design and implementation." SIAM Journal on Matrix Analysis and Applications 13.1 (1992): 333-356.
\end{document}
